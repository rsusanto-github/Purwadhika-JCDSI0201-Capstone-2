# -*- coding: utf-8 -*-
"""CAPSTONE2_JCDSI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q-6un9iULqsnZw5U6RVmHDuF3_hTKb8G

# **Library**
"""

#Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from scipy.stats import pearsonr

#Import csv from Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""# **Data**"""

#Show the Data
df = pd.read_csv('/content/drive/MyDrive/Transjakarta.csv')
display(df.head(),df.tail())

"""# **Data Cleaning**"""

# DATA CLEANING AND EXPLORATORY CHECKS

import pandas as pd
import numpy as np

#Basic info and missing values
print("=== Dataset Info ===")
df.info()

print("\n=== Missing Values per Column ===")
missing_values = df.isnull().sum().sort_values(ascending=False)
missing_percent = (missing_values / len(df) * 100).round(2)
missing_summary = pd.DataFrame({'Missing Values': missing_values, 'Percent %': missing_percent})
display(missing_summary)

#Handle missing payAmount (fill with 0)
df['payAmount'] = df['payAmount'].fillna(0)

#Check for duplicates
print("\n=== Duplicate Transactions ===")
duplicate_count = df.duplicated(subset=['transID']).sum()
print(f"Duplicate transID rows: {duplicate_count}")
df = df.drop_duplicates(subset=['transID'])

#Check data types and convert datetime
print("\n=== Data Types Before Conversion ===")
display(df.dtypes)

df['tapInTime'] = pd.to_datetime(df['tapInTime'], errors='coerce')
df['tapOutTime'] = pd.to_datetime(df['tapOutTime'], errors='coerce')

print("\n=== Data Types After Conversion ===")
display(df.dtypes)

#Compute trip duration in minutes
df['tripDurationMin'] = (df['tapOutTime'] - df['tapInTime']).dt.total_seconds() / 60

# Filter out negative or zero durations
df = df[df['tripDurationMin'] > 0]

#Outlier Detection (numerical columns)
numeric_cols = ['payAmount', 'tripDurationMin']
print("\n=== Outlier Detection ===")
for col in numeric_cols:
    if col in df.columns:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        outliers = df[(df[col] < lower) | (df[col] > upper)]
        print(f"{col}: {len(outliers)} outliers")
        # Optionally cap outliers
        df[col] = np.where(df[col] < lower, lower, df[col])
        df[col] = np.where(df[col] > upper, upper, df[col])

#Check for inconsistent categorical values
categorical_cols = ['payCardSex', 'direction', 'corridorName', 'tapInStopsName', 'tapOutStopsName']
for col in categorical_cols:
    print(f"\nUnique values in {col}: {df[col].unique()}")

#Summary after cleaning
print("\n=== Cleaned Dataset Info ===")
df.info()
display(df.head())

"""# **Handling Missing Values**"""

import pandas as pd
import numpy as np

# Handle corridor fields using whichever value is available
if 'corridorID' in df.columns and 'corridorName' in df.columns:
    df['corridorID'] = df['corridorID'].fillna(df['corridorName'])
    df['corridorName'] = df['corridorName'].fillna(df['corridorID'])
    df = df.dropna(subset=['corridorID', 'corridorName'], how='all')

# Median-filling for payment amounts
if 'payAmount' in df.columns:
    df['payAmount'] = df['payAmount'].fillna(df['payAmount'].median())

# Recover missing stop names using stop codes
if 'tapOutStopsName' in df.columns and 'tapOutStops' in df.columns:
    df['tapOutStopsName'] = df['tapOutStopsName'].fillna(df['tapOutStops'])

# Drop rows missing important tap-out information
needed_cols = ['tapOutStopsLat', 'tapOutStopsLon', 'stopEndSeq', 'tapOutTime']
existing_needed = [c for c in needed_cols if c in df.columns]
df = df.dropna(subset=existing_needed)

# Fill categorical values with "Unknown"
categorical_cols = [
    'tapInStops', 'tapOutStops', 'tapInStopsName', 'tapOutStopsName',
    'corridorName', 'payCardSex', 'direction', 'corridorID'
]
for col in categorical_cols:
    if col in df.columns:
        df[col] = df[col].fillna('Unknown')

# Fill remaining numerical values with 0
numeric_cols = ['payAmount']
for col in numeric_cols:
    if col in df.columns:
        df[col] = df[col].fillna(0)

# Fill missing coordinates with 0
coord_cols = [
    'tapInStopsLat', 'tapInStopsLon', 'tapOutStopsLat', 'tapOutStopsLon'
]
for col in coord_cols:
    if col in df.columns:
        df[col] = df[col].fillna(0)

# Display missing value summary
missing_count = df.isnull().sum()
missing_percent = (missing_count / len(df) * 100).round(2)

missing_summary = pd.DataFrame({
    'Missing Values': missing_count,
    'Percent %': missing_percent
}).sort_values(by='Missing Values', ascending=False)

display(missing_summary)

"""# **EDA**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option("display.max_columns", None)
sns.set(style="whitegrid")

# Load data before this cell

# Quick look at the dataset
display(df.head())
print("Shape:", df.shape)
print("\nMissing Values:\n", df.isna().sum())
print("\nData Types:\n", df.dtypes)

# Basic cleaning
df["tapInTime"]  = pd.to_datetime(df["tapInTime"], errors="coerce")
df["tapOutTime"] = pd.to_datetime(df["tapOutTime"], errors="coerce")
df["payCardBirthDate"] = pd.to_numeric(df["payCardBirthDate"], errors="coerce")

# Remove rows missing important route fields
df = df.dropna(subset=["tapInTime", "tapOutTime", "corridorID"])

# Replace negative payments
df["payAmount"] = df["payAmount"].apply(lambda x: max(x, 0) if pd.notnull(x) else x)

# Remove impossible latitudes
df = df[
    df["tapInStopsLat"].between(-90, 90) &
    df["tapOutStopsLat"].between(-90, 90)
]

print("Data after cleaning:", df.shape)

# Feature engineering
df["tripDurationMin"] = (df["tapOutTime"] - df["tapInTime"]).dt.total_seconds() / 60

current_year = 2025
df["age"] = current_year - df["payCardBirthDate"]

df["tapInHour"] = df["tapInTime"].dt.hour
df["tapInDay"]  = df["tapInTime"].dt.day_name()
df["noMovement"] = (df["tapInStops"] == df["tapOutStops"]).astype(int)

# Passenger trip counts
freq = df["payCardID"].value_counts()
df["passengerTripCount"] = df["payCardID"].map(freq)

from math import radians, cos, sin, asin, sqrt

def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dLat = radians(lat2 - lat1)
    dLon = radians(lat2 - lon1)
    lat1 = radians(lat1)
    lat2 = radians(lat2)
    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2
    return 2 * R * asin(sqrt(a))

df["tripDistanceKM"] = df.apply(
    lambda x: haversine(
        x["tapInStopsLat"], x["tapInStopsLon"],
        x["tapOutStopsLat"], x["tapOutStopsLon"]
    ), axis=1
)

df["avgSpeedKMH"] = (df["tripDistanceKM"] / (df["tripDurationMin"] / 60)).replace([np.inf, -np.inf], np.nan)

display(df.head())

# EDA

# Trip duration distribution
plt.figure(figsize=(8,5))
sns.histplot(df["tripDurationMin"], bins=60, kde=True)
plt.title("Trip Duration (Minutes)")
plt.show()

mean_duration = df["tripDurationMin"].mean()

# Hourly demand
plt.figure(figsize=(8,5))
df["tapInHour"].value_counts().sort_index().plot(kind="bar")
plt.title("Trips by Hour")
plt.show()

peak_hour = df["tapInHour"].value_counts().idxmax()

# Day of week
plt.figure(figsize=(8,5))
df["tapInDay"].value_counts().reindex(
    ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
).plot(kind="bar")
plt.title("Trips by Day")
plt.show()

# Corridor demand
plt.figure(figsize=(10,6))
df["corridorName"].value_counts().head(10).plot(kind="bar")
plt.title("Top 10 Corridors")
plt.show()

top_corridor = df["corridorName"].value_counts().idxmax()

# Revenue distribution
revenue = df["payAmount"].sum()
avg_payment = df["payAmount"].mean()

plt.figure(figsize=(8,5))
sns.boxplot(x=df["payAmount"])
plt.title("Fare Distribution")
plt.show()

# Distance vs duration
plt.figure(figsize=(7,5))
sns.scatterplot(x="tripDistanceKM", y="tripDurationMin", data=df, alpha=0.3)
plt.title("Distance vs Duration")
plt.show()

corr_eff = df["tripDistanceKM"].corr(df["tripDurationMin"])

# Speed distribution
plt.figure(figsize=(8,5))
sns.histplot(df["avgSpeedKMH"], bins=60, kde=True)
plt.title("Average Speed Distribution")
plt.show()

# Passenger frequency
plt.figure(figsize=(8,5))
sns.histplot(df["passengerTripCount"], bins=50)
plt.title("Passenger Trip Frequency")
plt.show()

heavy_users = (df["passengerTripCount"] > df["passengerTripCount"].median()).mean()

# OD pair analysis
df["OD_pair"] = df["tapInStopsName"] + " â†’ " + df["tapOutStopsName"]
top_od = df["OD_pair"].value_counts().head(10)

plt.figure(figsize=(10,6))
top_od.plot(kind="bar")
plt.title("Top 10 OD Pairs")
plt.show()

# Insights summary
print("\nSummary of Insights:\n")
print(f"- Average trip duration: {mean_duration:.2f} minutes")
print(f"- Busiest hour: {peak_hour}:00")
print(f"- Highest demand corridor: {top_corridor}")
print(f"- Total revenue: {revenue:,.2f}")
print(f"- Average payment per trip: {avg_payment:.2f}")
print(f"- Distanceâ€“duration correlation: {corr_eff:.2f}")
print(f"- {heavy_users*100:.1f}% of riders make more-than-median trips")
print("- OD pairs reveal the most common travel flows\n")

"""# **Corridor Analysis**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set blue theme for visualizations
sns.set(style="whitegrid", palette="Blues_d")

# 1. Feature preparation (assumes df is already loaded and cleaned)

# Compute trip duration in minutes
df["tripDurationMin"] = (df["tapOutTime"] - df["tapInTime"]).dt.total_seconds() / 60

# Compute trip distance with the Haversine formula if not already present
def haversine(lat1, lon1, lat2, lon2):
    from math import radians, sin, cos, asin, sqrt
    R = 6371
    dLat = radians(lat2 - lat1)
    dLon = radians(lon2 - lon1)
    lat1 = radians(lat1)
    lat2 = radians(lat2)
    a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2
    return 2 * R * asin(sqrt(a))

if "tripDistanceKM" not in df.columns:
    df["tripDistanceKM"] = df.apply(
        lambda x: haversine(
            x["tapInStopsLat"], x["tapInStopsLon"],
            x["tapOutStopsLat"], x["tapOutStopsLon"]
        ), axis=1
    )

# Compute average speed in km/h
df["avgSpeedKMH"] = df["tripDistanceKM"] / (df["tripDurationMin"] / 60)

# 2. Aggregate corridor performance statistics
corridor_stats = df.groupby("corridorName").agg(
    totalTrips = ("transID", "count"),
    avgDurationMin = ("tripDurationMin", "mean"),
    avgDistanceKM = ("tripDistanceKM", "mean"),
    avgSpeedKMH = ("avgSpeedKMH", "mean"),
    totalRevenue = ("payAmount", "sum"),
    uniqueUsers = ("payCardID", "nunique")
).reset_index()

corridor_stats = corridor_stats.sort_values(by="totalTrips", ascending=False)
display(corridor_stats.head(10))

# 3. Bar chart: Top 10 busiest corridors
plt.figure(figsize=(12,6))
sns.barplot(
    data=corridor_stats.head(10),
    x="totalTrips",
    y="corridorName",
    palette="Blues_r",
    edgecolor="black"
)
plt.title("Top 10 Busiest Corridors by Total Trips", color="#003366", fontsize=15)
plt.xlabel("Total Number of Trips")
plt.ylabel("Corridor")
plt.tight_layout()
plt.show()

# 4. Bar chart: Slowest corridors by average speed
speed_sorted = corridor_stats.sort_values(by="avgSpeedKMH")

plt.figure(figsize=(12,6))
sns.barplot(
    data=speed_sorted.head(10),
    x="avgSpeedKMH",
    y="corridorName",
    palette="Blues_r",
    edgecolor="black"
)
plt.title("10 Slowest Corridors (Lowest Avg Speed)", color="#003366", fontsize=15)
plt.xlabel("Average Speed (KM/H)")
plt.ylabel("Corridor")
plt.tight_layout()
plt.show()

# 5. Bar chart: Top revenue-generating corridors
revenue_sorted = corridor_stats.sort_values(by="totalRevenue", ascending=False)

plt.figure(figsize=(12,6))
sns.barplot(
    data=revenue_sorted.head(10),
    x="totalRevenue",
    y="corridorName",
    palette="Blues_r",
    edgecolor="black"
)
plt.title("Top 10 Revenue Generating Corridors", color="#003366", fontsize=15)
plt.xlabel("Total Revenue (Currency)")
plt.ylabel("Corridor")
plt.tight_layout()
plt.show()

# Spatial Heatmap of Stops (Folium)

import folium
from folium.plugins import HeatMap
import pandas as pd

# Make sure df is loaded and tapInLat/Lon exist
df['tapInStopsLat'] = pd.to_numeric(df['tapInStopsLat'], errors='coerce')
df['tapInStopsLon'] = pd.to_numeric(df['tapInStopsLon'], errors='coerce')

# Remove invalid coordinates
df_map = df.dropna(subset=['tapInStopsLat','tapInStopsLon'])

# Aggregate passenger counts per stop
stop_counts = df_map.groupby(['tapInStopsName','tapInStopsLat','tapInStopsLon']).size().reset_index(name='count')

# Prepare data for HeatMap: [lat, lon, weight]
heat_data = stop_counts[['tapInStopsLat','tapInStopsLon','count']].values.tolist()

# Center map
center_lat = stop_counts['tapInStopsLat'].mean()
center_lon = stop_counts['tapInStopsLon'].mean()

# Create map
m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

# Add heatmap layer
HeatMap(heat_data, radius=15, max_zoom=13).add_to(m)

# Optionally: add markers for stops
for _, row in stop_counts.iterrows():
    folium.CircleMarker(
        location=[row['tapInStopsLat'], row['tapInStopsLon']],
        radius=3,
        popup=f"{row['tapInStopsName']}: {row['count']} trips",
        color='blue',
        fill=True
    ).add_to(m)

# Display map
m

# Spatial Heatmap + Top 10 Corridor Labels


import folium
from folium.plugins import HeatMap
import pandas as pd

# Ensure coordinates are numeric
df['tapInStopsLat'] = pd.to_numeric(df['tapInStopsLat'], errors='coerce')
df['tapInStopsLon'] = pd.to_numeric(df['tapInStopsLon'], errors='coerce')

# Drop invalid coordinates
df_map = df.dropna(subset=['tapInStopsLat','tapInStopsLon'])

# Top 10 Corridors by Trips
corridor_stats = df.groupby('corridorName').agg(
    total_trips=('transID','count'),
    avg_duration=('tapOutTime','count'),  # placeholder, we will compute below
    total_revenue=('payAmount','sum')
).sort_values('total_trips', ascending=False).head(10)

# Compute avg duration
df['tripDurationMin'] = (df['tapOutTime'] - df['tapInTime']).dt.total_seconds() / 60
corridor_stats['avg_duration_min'] = df.groupby('corridorName')['tripDurationMin'].mean().loc[corridor_stats.index]

print("Top 10 most visited corridors and stats:")
display(corridor_stats)

# Aggregate stops for heatmap
stop_counts = df_map.groupby(['tapInStopsName','tapInStopsLat','tapInStopsLon']).size().reset_index(name='count')
heat_data = stop_counts[['tapInStopsLat','tapInStopsLon','count']].values.tolist()

# Map center
center_lat = stop_counts['tapInStopsLat'].mean()
center_lon = stop_counts['tapInStopsLon'].mean()

# Create map
m = folium.Map(location=[center_lat, center_lon], zoom_start=12)

# Add heatmap
HeatMap(heat_data, radius=15, max_zoom=13).add_to(m)

# Label top corridors
# For each top corridor, get median location of its stops to place a label
top_corridors = corridor_stats.index.tolist()
for corridor in top_corridors:
    corridor_stops = df_map[df_map['corridorName']==corridor]
    if corridor_stops.empty:
        continue
    median_lat = corridor_stops['tapInStopsLat'].median()
    median_lon = corridor_stops['tapInStopsLon'].median()
    stats = corridor_stats.loc[corridor]
    popup_text = (
        f"<b>{corridor}</b><br>"
        f"Total Trips: {stats['total_trips']}<br>"
        f"Avg Duration: {stats['avg_duration_min']:.1f} min<br>"
        f"Total Revenue: {stats['total_revenue']:.2f}"
    )
    folium.Marker(
        location=[median_lat, median_lon],
        popup=popup_text,
        icon=folium.Icon(color='red', icon='info-sign')
    ).add_to(m)

# Display map
m

# Top 10 Most Visited Corridors (List Only)

# Ensure trip duration is computed
df['tripDurationMin'] = (df['tapOutTime'] - df['tapInTime']).dt.total_seconds() / 60

# Aggregate corridor stats
corridor_stats = df.groupby('corridorName').agg(
    total_trips=('transID','count'),
    avg_duration=('tripDurationMin','mean'),
    total_revenue=('payAmount','sum')
).sort_values('total_trips', ascending=False).head(10)

# Display the top 10 as a list
print("=== Top 10 Most Visited Corridors ===\n")
for i, (corridor, stats) in enumerate(corridor_stats.iterrows(), 1):
    print(f"{i}. {corridor}")
    print(f"   - Total Trips: {stats['total_trips']}")
    print(f"   - Average Trip Duration: {stats['avg_duration']:.1f} min")
    print(f"   - Total Revenue: {stats['total_revenue']:.2f}\n")

# Corridor Performance Summary (All Corridors)

# Ensure duration column exists
df['tripDurationMin'] = (df['tapOutTime'] - df['tapInTime']).dt.total_seconds() / 60

# Compute corridor statistics
corridor_stats = df.groupby('corridorName').agg(
    total_trips=('transID', 'count'),
    avg_duration=('tripDurationMin', 'mean'),
    total_revenue=('payAmount', 'sum')
).sort_values('total_trips', ascending=False)

print("=== All Corridor Performance Statistics ===\n")

for i, (corridor, stats) in enumerate(corridor_stats.iterrows(), 1):
    print(f"{i}. {corridor}")
    print(f"   - Total Trips: {stats['total_trips']}")
    print(f"   - Average Trip Duration: {stats['avg_duration']:.1f} min")
    print(f"   - Total Revenue: {stats['total_revenue']:.2f}\n")

"""# **Feature Engineering For Corridor Analysis**

# Engineered Features Table

| Feature Name | Description | Dataset Used / Combined With |
|---|---|---|
| visit_count | Total number of visits to the corridor | Trajectory dataset |
| unique_visitors | Number of distinct user/device IDs visiting the corridor | Trajectory dataset |
| avg_time_spent | Average time spent per visit | Trajectory dataset |
| median_time_spent | Median session duration | Trajectory dataset |
| peak_hour | Hour with highest number of visitors | Trajectory dataset |
| weekend_usage_ratio | % of traffic happening on weekends | Trajectory dataset |
| revisit_rate | Returning visitors ratio | Trajectory dataset |
| avg_speed | Distance divided by time spent | Trajectory + indoor map |
| congestion_score | Visitors per sqm | Trajectory + corridor dimensions |


*   List item
*   List item


"""

# ==============================================================
# FEATURE ENGINEERING FOR CORRIDOR PERFORMANCE ANALYSIS (12 METRICS)
# ==============================================================

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ==============================
# 1. Datetime & Trip Duration
# ==============================
df['tapInTime'] = pd.to_datetime(df['tapInTime'])
df['tapOutTime'] = pd.to_datetime(df['tapOutTime'])
df['tripDurationMin'] = (df['tapOutTime'] - df['tapInTime']).dt.total_seconds() / 60
df['tripDate'] = df['tapInTime'].dt.date
df['tripHour'] = df['tapInTime'].dt.hour
df['tripWeekday'] = df['tapInTime'].dt.weekday  # 0=Mon, 6=Sun

# Indicators
df['isFree'] = df['payAmount'].apply(lambda x: 1 if x == 0 else 0)
df['isWeekend'] = df['tripWeekday'].apply(lambda x: 1 if x >= 5 else 0)

# ==============================
# 2. Corridor Base Features (12 Metrics)
# ==============================
corridor_features = df.groupby('corridorName').agg(
    total_trips=('transID', 'count'),
    avg_trip_duration=('tripDurationMin', 'mean'),
    median_trip_duration=('tripDurationMin', 'median'),
    std_trip_duration=('tripDurationMin', 'std'),
    total_revenue=('payAmount', 'sum'),
    avg_revenue_per_trip=('payAmount', 'mean'),
    unique_passengers=('payCardID', 'nunique'),
    total_free_rides=('isFree', 'sum'),
    free_ride_ratio=('isFree', 'mean'),
    weekend_usage_ratio=('isWeekend', 'mean')
)

# Average trips per active day
days_active = df.groupby('corridorName')['tripDate'].nunique()
corridor_features['avg_trips_per_day'] = corridor_features['total_trips'] / days_active

# Peak hour
peak_hours = (
    df.groupby(['corridorName', 'tripHour'])['transID']
      .count()
      .reset_index()
      .sort_values(['corridorName', 'transID'], ascending=[True, False])
      .groupby('corridorName')
      .first()
)
corridor_features['peak_hour'] = peak_hours['tripHour']

# Revisit rate
user_visits = df.groupby(['corridorName', 'payCardID']).size().reset_index(name='visits')
revisit = user_visits[user_visits['visits'] > 1].groupby('corridorName')['payCardID'].nunique()
corridor_features['revisit_rate'] = revisit / corridor_features['unique_passengers']
corridor_features['revisit_rate'] = corridor_features['revisit_rate'].fillna(0)

# ==============================
# 3. Performance Score
# ==============================
scaler = MinMaxScaler()
corridor_features['performance_score'] = (
    scaler.fit_transform(corridor_features[['total_trips']])[:,0] * 0.6 +
    scaler.fit_transform(-corridor_features[['avg_trip_duration']])[:,0] * 0.4
)

# Sort corridors
corridor_features = corridor_features.sort_values('performance_score', ascending=False)

# ==============================
# 4. Display
# ==============================
print("=== Corridor Performance Feature Table ===\n")
display(corridor_features)

# ==============================================================
# DISPLAY TOP 10 AND BOTTOM 10 PERFORMING CORRIDORS
# ==============================================================

# Top 10 highest performing corridors
print("=== Top 10 Highest Performing Corridors ===\n")
top10 = corridor_features.head(10)
for i, (name, row) in enumerate(top10.iterrows(), 1):
    print(f"{i}. {name}")
    print(f"   - Total Trips: {row['total_trips']}")
    print(f"   - Avg Duration: {row['avg_trip_duration']:.1f} min")
    print(f"   - Median Duration: {row['median_trip_duration']:.1f} min")
    print(f"   - Std Duration: {row['std_trip_duration']:.1f} min")
    print(f"   - Total Revenue: {row['total_revenue']:.2f}")
    print(f"   - Avg Revenue per Trip: {row['avg_revenue_per_trip']:.2f}")
    print(f"   - Unique Passengers: {row['unique_passengers']}")
    print(f"   - Total Free Rides: {row['total_free_rides']}")
    print(f"   - Free Ride Ratio: {row['free_ride_ratio']:.2f}")
    print(f"   - Weekend Usage Ratio: {row['weekend_usage_ratio']:.2f}")
    print(f"   - Avg Trips per Day: {row['avg_trips_per_day']:.1f}")
    print(f"   - Peak Hour: {int(row['peak_hour'])}:00")
    print(f"   - Performance Score: {row['performance_score']:.3f}\n")

# Bottom 10 lowest performing corridors
print("=== Bottom 10 Lowest Performing Corridors ===\n")
bottom10 = corridor_features.tail(10)
for i, (name, row) in enumerate(bottom10.iterrows(), 1):
    print(f"{i}. {name}")
    print(f"   - Total Trips: {row['total_trips']}")
    print(f"   - Avg Duration: {row['avg_trip_duration']:.1f} min")
    print(f"   - Median Duration: {row['median_trip_duration']:.1f} min")
    print(f"   - Std Duration: {row['std_trip_duration']:.1f} min")
    print(f"   - Total Revenue: {row['total_revenue']:.2f}")
    print(f"   - Avg Revenue per Trip: {row['avg_revenue_per_trip']:.2f}")
    print(f"   - Unique Passengers: {row['unique_passengers']}")
    print(f"   - Total Free Rides: {row['total_free_rides']}")
    print(f"   - Free Ride Ratio: {row['free_ride_ratio']:.2f}")
    print(f"   - Weekend Usage Ratio: {row['weekend_usage_ratio']:.2f}")
    print(f"   - Avg Trips per Day: {row['avg_trips_per_day']:.1f}")
    print(f"   - Peak Hour: {int(row['peak_hour'])}:00")
    print(f"   - Performance Score: {row['performance_score']:.3f}\n")

import matplotlib.pyplot as plt
import seaborn as sns

# ==============================
# Prepare Data
# ==============================
top10 = corridor_features.head(10).copy()
bottom10 = corridor_features.tail(10).copy()

# ==============================
# Plot Top 10 Corridors
# ==============================
plt.figure(figsize=(16, 6))
sns.barplot(x='performance_score', y=top10.index, data=top10, palette="rainbow")
plt.title("Top 10 Highest Performing Corridors (Performance Score)", fontsize=16)
plt.xlabel("Performance Score", fontsize=12)
plt.ylabel("Corridor", fontsize=12)
plt.show()

"""# **Corridor Grouping & Assignment**


"""

# CORRIDOR GROUPING BASED ON PERFORMANCE

# Define a function to assign corridor groups
def corridor_group(row):
    if row['total_trips'] > 300 and row['avg_trip_duration'] < 72:
        return 'High Demand â€“ High Efficiency'
    elif row['total_trips'] > 300 and row['avg_trip_duration'] >= 72:
        return 'High Demand â€“ Low Efficiency'
    elif 250 <= row['total_trips'] <= 300:
        return 'Medium Demand â€“ Medium Efficiency'
    else:
        return 'Low Demand â€“ Low Efficiency'

# Apply the function to create a new 'group' column
corridor_features['group'] = corridor_features.apply(corridor_group, axis=1)

# Display group assignment
print("=== Corridor Group Assignment ===\n")
display(corridor_features[['total_trips','avg_trip_duration','peak_hour','group']])

# OPTIONAL: Summarize each group
group_summary = corridor_features.groupby('group').agg(
    num_corridors=('total_trips','count'),
    avg_total_trips=('total_trips','mean'),
    avg_duration=('avg_trip_duration','mean'),
    avg_peak_hour=('peak_hour','mean'),
    avg_revisit_rate=('revisit_rate','mean'),
    avg_weekend_ratio=('weekend_usage_ratio','mean')
).sort_values('avg_total_trips', ascending=False)

print("\n=== Corridor Group Summary ===\n")
display(group_summary)

# OPTIONAL: Print business plan suggestions per group
business_plan = {
    'High Demand â€“ High Efficiency': 'Maintain frequency & reliability, loyalty programs, explore premium services.',
    'High Demand â€“ Low Efficiency': 'Reduce congestion, optimize schedule, implement express services.',
    'Medium Demand â€“ Medium Efficiency': 'Boost marketing, optimize peak hours, improve weekend engagement.',
    'Low Demand â€“ Low Efficiency': 'Reassess route viability, explore feeder services or merged corridors.'
}

print("\n=== Suggested Business Plan per Group ===\n")
for group, plan in business_plan.items():
    print(f"{group}: {plan}\n")

from google.colab import files
df.to_csv("cleaned_data.csv", index=False)
files.download("cleaned_data.csv")

"""# **Problem & Solution**


# ðŸ“Œ Corridor Performance â€“ Problem and Solution

## Problem Statement

| Group | Problem |
|-------|---------|
| High Demand â€“ Low Efficiency | High passenger volume (~334 trips) but long travel durations (~74 min) and moderate loyalty (~38%). Congestion during peak hours reduces service efficiency. |
| High Demand â€“ High Efficiency | High passenger volume (~322 trips) with slightly lower trip durations (~71 min), moderate loyalty (~37%). Needs to maintain reliability and service quality. |
| Medium Demand â€“ Medium Efficiency | Moderate traffic (~268 trips) with long travel durations (~73 min) and moderate loyalty (~36%). Ridership growth is limited and weekend usage is low. |
| Low Demand â€“ Low Efficiency | Low ridership (~143 trips), long travel durations (~72 min), low loyalty (~26%). Slightly higher weekend usage (~13%), but overall underutilized corridor capacity. |

## Solutions

| Group | Solution |
|-------|---------|
| High Demand â€“ High Efficiency | Maintain frequency & reliability, implement loyalty programs, explore premium or express services. |
| High Demand â€“ Low Efficiency | Reduce congestion by optimizing schedules, add express services, improve fleet allocation during peak hours. |
| Medium Demand â€“ Medium Efficiency | Boost marketing to increase ridership, optimize peak-hour scheduling, introduce weekend engagement initiatives. |
| Low Demand â€“ Low Efficiency | Reassess route viability, consolidate or merge underperforming corridors, explore feeder services to increase usage. |

"""